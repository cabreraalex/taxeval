{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize TaxEval with Zeno\n",
    "\n",
    "Visualize the base data and model outputs of TaxEval using [Zeno](https://zenoml.com).\n",
    "\n",
    "* [Example project](https://hub.zenoml.com/project/e7519c8a-3c07-45ce-90f4-3a5ba4660ab6/LLM%20Taxes%20Benchmark)\n",
    "* [Example report](https://hub.zenoml.com/report/1717/Can%20LLMs%20do%20Your%20Taxes%3F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "from zeno_client import ZenoClient, ZenoMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('output.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(input):\n",
    "    return_question = input['source_question']['description'].replace('\\\\n', '\\n')\n",
    "    return_question += '\\n\\n'\n",
    "    for answer in enumerate(input['source_question']['options']):\n",
    "        return_question += f\"{answer[0] + 1}. {answer[1]}\\n\"\n",
    "    return return_question\n",
    "\n",
    "df_input = pd.DataFrame({\n",
    "    \"question\": [format_question(d) for d in data],\n",
    "    \"answer\": [str(d['source_question']['correct_answer']) for d in data],\n",
    "    \"reference\": [d['source_question']['reference'] for d in data],\n",
    "    \"tag\": [d['source_question']['tag'] for d in data],\n",
    "    \"category\": [d['source_question']['category'] for d in data],\n",
    "})\n",
    "df_input['question length'] = df_input['question'].apply(lambda x: len(x))\n",
    "df_input['id'] = df_input.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional, generate topics using BERTopic\n",
    "\n",
    "# from bertopic import BERTopic\n",
    "\n",
    "# topic_model = BERTopic(\"english\", min_topic_size=3)\n",
    "# topics, probs = topic_model.fit_transform([d['source_question']['description'] for d in data])\n",
    "# df_input['topic'] = topics\n",
    "# df_input['topic'] = df_input['topic'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Zeno Project\n",
    "\n",
    "Create an account at [hub.zenoml.com](https://hub.zenoml.com) and go to your [account page](https://hub.zenoml.com/account) to get your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(override=True)\n",
    "\n",
    "zeno_client = ZenoClient(os.environ.get(\"ZENO_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = zeno_client.create_project(\n",
    "    name=\"LLM Taxes Benchmark\", \n",
    "    view={\n",
    "        \"data\": {\n",
    "            \"type\": \"markdown\"\n",
    "        },\n",
    "        \"label\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"type\": \"markdown\"\n",
    "        } \n",
    "    },\n",
    "    description=\"Tax questions for LLMs\",\n",
    "    public=True,\n",
    "    metrics=[\n",
    "        ZenoMetric(name=\"accuracy\", type=\"mean\", columns=[\"correct\"]),\n",
    "        ZenoMetric(name=\"output length\", type=\"mean\", columns=[\"output length\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.upload_dataset(df_input, id_column=\"id\", data_column=\"question\", label_column=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in data[0]['full'].keys():\n",
    "    df_system = pd.DataFrame({\n",
    "        \"output\": [f\"**Full:** {d['full'][model]}\\n\\n**Simplified**: {d['simplified'][model]}\" for d in data],\n",
    "        \"output length\": [len(d['full'][model]) for d in data], \n",
    "        \"simplified output\": [str(d['simplified'][model]) for d in data],\n",
    "    })\n",
    "    df_system['correct'] = df_input['answer'] == df_system['simplified output']\n",
    "    df_system['id'] = df_input['id']\n",
    "    project.upload_system(df_system, name=model.replace('/', '-'), id_column=\"id\", output_column=\"output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
